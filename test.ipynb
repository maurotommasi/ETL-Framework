{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing for https://www.nytimes.com\n",
      "html\n",
      "Start extraction for https://www.nytimes.com\n",
      "Extraction \"https://www.nytimes.com\"...\n",
      "Queuing for https://www.theguardian.com\n",
      "html\n",
      "Start extraction for https://www.theguardian.com\n",
      "Extraction \"https://www.theguardian.com\"...\n",
      "Queuing for https://www.wsj.com\n",
      "html\n",
      "Start extraction for https://www.wsj.com\n",
      "Extraction \"https://www.wsj.com\"...\n",
      "Queuing for https://www.washingtonpost.com\n",
      "html\n",
      "Start extraction for https://www.washingtonpost.com\n",
      "Extraction \"https://www.washingtonpost.com\"...\n",
      "Queuing for https://www.usatoday.com\n",
      "html\n",
      "Start extraction for https://www.usatoday.com\n",
      "Extraction \"https://www.usatoday.com\"...\n",
      "Queuing for https://www.latimes.com\n",
      "html\n",
      "Start extraction for https://www.latimes.com\n",
      "Extraction \"https://www.latimes.com\"...\n",
      "Queuing for https://www.chicagotribune.com\n",
      "html\n",
      "Start extraction for https://www.chicagotribune.com\n",
      "Extraction \"https://www.chicagotribune.com\"...\n",
      "Queuing for https://www.boston.com\n",
      "html\n",
      "Start extraction for https://www.boston.com\n",
      "Extraction \"https://www.boston.com\"...\n",
      "Queuing for https://www.independent.co.uk\n",
      "html\n",
      "Start extraction for https://www.independent.co.uk\n",
      "Extraction \"https://www.independent.co.uk\"...\n",
      "Queuing for https://www.theaustralian.com.au\n",
      "html\n",
      "Start extraction for https://www.theaustralian.com.au\n",
      "Extraction \"https://www.theaustralian.com.au\"...\n",
      "Error: Unable to fetch the webpage. Status code: 403\n",
      "Error: local variable 'e' referenced before assignment\n",
      "Extraction \"https://www.boston.com\" completed.\n",
      "Saving/Uploading \"https://www.boston.com\"...\n",
      "\"https://www.boston.com\" correctly saved/uploaded\n",
      "Extraction \"https://www.chicagotribune.com\" completed.\n",
      "Saving/Uploading \"https://www.chicagotribune.com\"...\n",
      "Extraction \"https://www.independent.co.uk\" completed.\n",
      "Saving/Uploading \"https://www.independent.co.uk\"...\n",
      "Error: Unable to fetch the webpage. Status code: 403\n",
      "\"https://www.chicagotribune.com\" correctly saved/uploaded\n",
      "Error: local variable 'e' referenced before assignment\n",
      "\"https://www.independent.co.uk\" correctly saved/uploaded\n",
      "Extraction \"https://www.nytimes.com\" completed.\n",
      "Saving/Uploading \"https://www.nytimes.com\"...\n",
      "Extraction \"https://www.usatoday.com\" completed.\n",
      "Saving/Uploading \"https://www.usatoday.com\"...\n",
      "\"https://www.nytimes.com\" correctly saved/uploaded\n",
      "\"https://www.usatoday.com\" correctly saved/uploaded\n",
      "Extraction \"https://www.theguardian.com\" completed.\n",
      "Saving/Uploading \"https://www.theguardian.com\"...\n",
      "\"https://www.theguardian.com\" correctly saved/uploaded\n",
      "Extraction \"https://www.theaustralian.com.au\" completed.\n",
      "Saving/Uploading \"https://www.theaustralian.com.au\"...\n",
      "\"https://www.theaustralian.com.au\" correctly saved/uploaded\n",
      "Extraction \"https://www.washingtonpost.com\" completed.\n",
      "Saving/Uploading \"https://www.washingtonpost.com\"...\n",
      "\"https://www.washingtonpost.com\" correctly saved/uploaded\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE FOR WEB SCRAPING\n",
    "# In this case the mapping file example to follow is: mapping_source_html.json, inside we have\n",
    "# {\n",
    "#     \"mapping\": [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"a\", \"img\"]\n",
    "# }\n",
    "# Inside the list we can put the list of tags we want to fetch\n",
    "# This file will be passed to the encoder\n",
    "\n",
    "from ETL import ETL\n",
    "\n",
    "# Inizialize main istance ETL\n",
    "\n",
    "ETL_istance = ETL(\"Process ETL 1\")\n",
    "\n",
    "newspaper_websites = [\n",
    "    \"https://www.nytimes.com\",\n",
    "    \"https://www.theguardian.com\",\n",
    "    \"https://www.wsj.com\",\n",
    "    \"https://www.washingtonpost.com\",\n",
    "    \"https://www.usatoday.com\",\n",
    "    \"https://www.latimes.com\",\n",
    "    \"https://www.chicagotribune.com\",\n",
    "    \"https://www.boston.com\",\n",
    "    \"https://www.independent.co.uk\",\n",
    "    \"https://www.theaustralian.com.au\"\n",
    "]\n",
    "\n",
    "# Initialize sub istances\n",
    "\n",
    "source = ETL_istance.Source('html') # html is the name of the Source\n",
    "\n",
    "encoder = source.Encoder()\n",
    "data_source = source.DataSource()\n",
    "\n",
    "# Set encoder\n",
    "\n",
    "encoder.set_html(mapping_url=\"mapping/mapping_source_html.json\")\n",
    "\n",
    "# Create Save Object - This will help us to save/upload the info as specified in the dictionary\n",
    "# Each extraction has his own Save Object Dictionary (SOD)\n",
    "\n",
    "save_obj = {\n",
    "    'type': 'json',\n",
    "    'path': \"data.json\"\n",
    "}\n",
    "\n",
    "for url in newspaper_websites:\n",
    "    # Set Data Source Connection It can be an URL, a MYSQL Connection and so on\n",
    "    data_source.set_url(url)\n",
    "\n",
    "    # Set Source Object (Data Source + Enconder)\n",
    "    source.set_data_source(data_source, encoder)\n",
    "\n",
    "    # Extract and Load the Information in a queue of threads. Every Extraction is a indipendent thread\n",
    "    extraction = ETL.Extract(name=url,source=source)\n",
    "    extraction.queue(save_obj, 3600)\n",
    "\n",
    "# Here is the last piece of code that has to be run\n",
    "# This will allow to create N threads based on how many extraction were created\n",
    "# Is it possible to add multiple extractions from multiple ETL istances\n",
    "# Example:\n",
    "# ETL_istance_1 => Web scraping\n",
    "# ETL_istance_2 => Sync data from Google Cloud\n",
    "# ETL_istance_3 => Sync data from AWS EC2\n",
    "# This code will start the ETL process, be aware about IP blovking a request limits by the provider \n",
    "\n",
    "ETL_istance.Utils().runThreads([extraction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "running_threads = threading.enumerate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
